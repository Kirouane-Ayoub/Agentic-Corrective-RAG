{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNOMBsozNcet"
      },
      "outputs": [],
      "source": [
        "%pip install llama-index  llama-index-llms-cohere llama-index-embeddings-cohere llama_agents langchain_community duckduckgo_search"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "oiPqGAfBXh_J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initilise with your api key\n",
        "import os\n",
        "\n",
        "cohere_api_key = \"\"\n",
        "os.environ[\"COHERE_API_KEY\"] = cohere_api_key\n"
      ],
      "metadata": {
        "id": "YYPNu9v6Nr8T"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.cohere import CohereEmbedding\n",
        "from llama_index.llms.cohere import Cohere\n",
        "from llama_index.core import Settings\n",
        "\n",
        "llm = Cohere(api_key=cohere_api_key, model=\"command-r-plus\")\n",
        "\n",
        "# with input_typ='search_query'\n",
        "embed_model = CohereEmbedding(\n",
        "    api_key=cohere_api_key,\n",
        "    model_name=\"embed-english-v3.0\",\n",
        "    input_type=\"search_query\",\n",
        ")\n",
        "# global\n",
        "Settings.embed_model = embed_model\n",
        "Settings.llm = llm\n"
      ],
      "metadata": {
        "id": "XVeYnEqgNyc5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import  ServiceContext\n",
        "\n",
        "# Create the service context with the cohere model for generation and embedding model\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DioriS84N45x",
        "outputId": "ca0b1017-0d52-4625-b131-eff09c6236c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-45a44ac845c6>:4: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import  SimpleDirectoryReader\n",
        "\n",
        "docs = SimpleDirectoryReader(\"/content/data\").load_data()"
      ],
      "metadata": {
        "id": "Nf38O2XNOFJc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from llama_index.core import (\n",
        "    StorageContext,\n",
        "    VectorStoreIndex,\n",
        "    load_index_from_storage,\n",
        ")\n",
        "\n",
        "# build index\n",
        "index = VectorStoreIndex.from_documents(docs ,service_context=service_context )\n",
        "retriever = index.as_retriever()"
      ],
      "metadata": {
        "id": "jA63VoAJOt_4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchResults\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def duckduckgo_search(query:str) -> str :\n",
        "  search = DuckDuckGoSearchResults(max_results=15)\n",
        "  return search.run(query)"
      ],
      "metadata": {
        "id": "MZ4yFfxySoFa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_agents import ComponentService, ServiceComponent, SimpleMessageQueue\n",
        "\n",
        "message_queue = SimpleMessageQueue()\n",
        "\n",
        "\n",
        "def to_service_component(component, message_queue, service_name, description):\n",
        "    server = ComponentService(\n",
        "        component=component,\n",
        "        message_queue=message_queue,\n",
        "        description=description,\n",
        "        service_name=service_name,\n",
        "    )\n",
        "    service_component = ServiceComponent.from_component_service(server)\n",
        "    return service_component, server"
      ],
      "metadata": {
        "id": "wqUXpIL6T0uX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.core.query_pipeline import QueryPipeline\n",
        "\n",
        "relevancy_prompt_tmpl = PromptTemplate(\n",
        "    template=\"\"\"As a grader, your task is to evaluate the relevance of a document retrieved in response to a user's question.\n",
        "\n",
        "    Retrieved Document:\n",
        "    -------------------\n",
        "    {context_str}\n",
        "\n",
        "    User Question:\n",
        "    --------------\n",
        "    {query_str}\n",
        "\n",
        "    Evaluation Criteria:\n",
        "    - Consider whether the document contains keywords or topics related to the user's question.\n",
        "    - The evaluation should not be overly stringent; the primary objective is to identify and filter out clearly irrelevant retrievals.\n",
        "\n",
        "    Decision:\n",
        "    - Assign a binary score to indicate the document's relevance.\n",
        "    - Use 'yes' if the document is relevant to the question, or 'no' if it is not.\n",
        "\n",
        "    Please provide your binary score ('yes' or 'no') below to indicate the document's relevance to the user question.\"\"\"\n",
        ")\n",
        "relevancy_qp = QueryPipeline(chain=[relevancy_prompt_tmpl, llm])"
      ],
      "metadata": {
        "id": "tMbU0VHFT-ge"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define RAG agent\n",
        "from llama_index.core.query_pipeline import FnComponent\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "def run_retrieval(input_str: str) -> Dict:\n",
        "    \"\"\"Run Retrieval.\"\"\"\n",
        "    # retrieves a set of nodes\n",
        "    retrieved_nodes = retriever.retrieve(input_str)\n",
        "\n",
        "    # runs a relevancy check\n",
        "    relevancy_results = []\n",
        "    for node in retrieved_nodes:\n",
        "        relevancy = relevancy_qp.run(context_str=node.text, query_str=query_str)\n",
        "        relevancy_results.append(relevancy.message.content.lower().strip())\n",
        "    contains_irrelevant = \"no\" in relevancy_results\n",
        "\n",
        "    # get relevant texts\n",
        "    relevant_texts = [\n",
        "        retrieved_nodes[i].text\n",
        "        for i, result in enumerate(relevancy_results)\n",
        "        if result == \"yes\"\n",
        "    ]\n",
        "    relevant_text = \"\\n\".join(relevant_texts)\n",
        "\n",
        "    # returns a dictionary of items\n",
        "    return {\n",
        "        \"relevant_text\": relevant_text,\n",
        "        \"contains_irrelevant\": contains_irrelevant,\n",
        "        \"input_str\": input_str,\n",
        "    }\n",
        "\n",
        "\n",
        "retrieval_component = FnComponent(fn=run_retrieval)\n",
        "retrieval_component_s, retrieval_server = to_service_component(\n",
        "    retrieval_component,\n",
        "    message_queue,\n",
        "    \"Runs a retrieval + relevancy check\",\n",
        "    \"retrieval_service\",\n",
        ")"
      ],
      "metadata": {
        "id": "QNPeuvo2UJM2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.prompts import PromptTemplate\n",
        "\n",
        "query_transform_tmpl = PromptTemplate(\n",
        "    template=\"\"\"Your task is to refine a query to ensure it is highly effective for retrieving relevant search results. \\n\n",
        "    Analyze the given input to grasp the core semantic intent or meaning. \\n\n",
        "    Original Query:\n",
        "    \\n ------- \\n\n",
        "    {query_str}\n",
        "    \\n ------- \\n\n",
        "    Your goal is to rephrase or enhance this query to improve its search performance. Ensure the revised query is concise and directly aligned with the intended search objective. \\n\n",
        "    Respond with the optimized query only:\"\"\"\n",
        ")\n",
        "query_transform_qp = QueryPipeline(chain=[query_transform_tmpl, llm])"
      ],
      "metadata": {
        "id": "-RJjZUi8Ufrg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_web_search(input_str: str) -> str:\n",
        "    \"\"\"Run Web Search.\"\"\"\n",
        "\n",
        "    transformed_query_str = query_transform_qp.run(query_str=input_str).message.content\n",
        "    # Conduct a search with the transformed query string and collect the results.\n",
        "    search_results = duckduckgo_search(transformed_query_str)\n",
        "    return search_results\n",
        "\n",
        "\n",
        "web_search_component = FnComponent(fn=run_web_search)\n",
        "web_search_component_s, web_server = to_service_component(\n",
        "    web_search_component, message_queue, \"Runs web search\", \"web_search_service\"\n",
        ")"
      ],
      "metadata": {
        "id": "PESp3NKQUns-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.base.response.schema import Response\n",
        "from llama_index.core.schema import Document\n",
        "from llama_index.core import SummaryIndex\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "def run_summarization(retrieved_text: str, search_text: Optional[str] = None) -> str:\n",
        "    \"\"\"Run summarization.\"\"\"\n",
        "    # use summary index to perform summarization\n",
        "    search_text = search_text or \"\"\n",
        "    documents = [Document(text=retrieved_text + \"\\n\" + search_text)]\n",
        "    index = SummaryIndex.from_documents(documents)\n",
        "    query_engine = index.as_query_engine()\n",
        "    return str(query_engine.query(query_str))\n",
        "\n",
        "\n",
        "summary_component = FnComponent(fn=run_summarization)\n",
        "summary_component_s, summary_server = to_service_component(\n",
        "    summary_component, message_queue, \"Run summarization\", \"summarization_service\"\n",
        ")"
      ],
      "metadata": {
        "id": "L_FrKLXAV6_T"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_agents import (\n",
        "    AgentService,\n",
        "    ControlPlaneServer,\n",
        "    SimpleMessageQueue,\n",
        "    PipelineOrchestrator,\n",
        "    ServiceComponent,\n",
        "    ComponentService,\n",
        ")\n",
        "from llama_index.core.query_pipeline import Link, InputComponent\n",
        "\n",
        "pipeline = QueryPipeline(\n",
        "    module_dict={\n",
        "        \"input\": InputComponent(),\n",
        "        \"retrieval_server\": retrieval_component_s,\n",
        "        \"web_server\": web_search_component_s,\n",
        "        \"summary_server_no_web\": summary_component_s,\n",
        "        \"summary_server_web\": summary_component_s,\n",
        "    }\n",
        ")\n",
        "pipeline.add_link(\"input\", \"retrieval_server\")\n",
        "pipeline.add_link(\n",
        "    \"retrieval_server\",\n",
        "    \"web_server\",\n",
        "    condition_fn=lambda x: x[\"contains_irrelevant\"],\n",
        "    input_fn=lambda x: x[\"input_str\"],\n",
        ")\n",
        "# if web search is called\n",
        "pipeline.add_link(\n",
        "    \"retrieval_server\",\n",
        "    \"summary_server_web\",\n",
        "    dest_key=\"retrieved_text\",\n",
        "    condition_fn=lambda x: x[\"contains_irrelevant\"],\n",
        "    input_fn=lambda x: x[\"relevant_text\"],\n",
        ")\n",
        "pipeline.add_link(\"web_server\", \"summary_server_web\", dest_key=\"search_text\")\n",
        "\n",
        "# if web search is not called\n",
        "pipeline.add_link(\n",
        "    \"retrieval_server\",\n",
        "    \"summary_server_no_web\",\n",
        "    dest_key=\"retrieved_text\",\n",
        "    condition_fn=lambda x: not x[\"contains_irrelevant\"],\n",
        "    input_fn=lambda x: x[\"relevant_text\"],\n",
        ")\n",
        "\n",
        "pipeline_orchestrator = PipelineOrchestrator(pipeline)\n",
        "\n",
        "control_plane = ControlPlaneServer(\n",
        "    message_queue=message_queue,\n",
        "    orchestrator=pipeline_orchestrator,\n",
        ")"
      ],
      "metadata": {
        "id": "7C_6fuGTWIv0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_agents.launchers import LocalLauncher\n",
        "\n",
        "## Define Launcher\n",
        "launcher = LocalLauncher(\n",
        "    [retrieval_server, web_server, summary_server],\n",
        "    control_plane,\n",
        "    message_queue,\n",
        ")"
      ],
      "metadata": {
        "id": "_S8q8CPoWufk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    query_str = input(\"Enter query: \")\n",
        "    result = launcher.launch_single(query_str)\n",
        "    print(str(result))"
      ],
      "metadata": {
        "id": "6K-IDqprXPAv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
